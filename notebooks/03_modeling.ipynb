{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modeling\n",
    "\n",
    "In this notebook, we will build the recommendation models using the prepared datasets.\n",
    "We will be using the Surprise library to implement both SVD and KNNBaseline algorithms.\n",
    "The choice of these algorithms is based on their effectiveness in collaborative filtering tasks with minimal preprocessing requirements."
   ],
   "id": "83b5de598428b33e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Importing Libraries\n",
    "\n",
    "In this cell, we will import all the necessary libraries and functions for model building and data loading from the disk."
   ],
   "id": "e2f3fd1eb9e56150"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:10.543694514Z",
     "start_time": "2026-01-07T19:25:10.512278814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from surprise import SVD, KNNBaseline\n",
    "import pickle as plk\n",
    "from pathlib import Path as pth\n",
    "import IPython\n",
    "import gc"
   ],
   "id": "b043f32a57655a14",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Loading Prepared Data from the previous notebook\n",
    "\n",
    "In this cell, we will load the prepared training from disk using the pickle format.\n",
    "This will allow us to easily access the datasets for model training while avoiding redundant data preparation steps."
   ],
   "id": "f7dd7f86cd11b1dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:10.748968557Z",
     "start_time": "2026-01-07T19:25:10.544793226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the paths to load the prepared datasets\n",
    "train_1m_path = pth.cwd().parent / 'data' / 'prepared-1m' / 'train_1m.pkl'\n",
    "train_100k_path = pth.cwd().parent / 'data' / 'prepared-100k' / 'train_100k.pkl'\n",
    "\n",
    "# Loading the 1M training dataset\n",
    "with open(train_1m_path, 'rb') as f:\n",
    "    train_1m = plk.load(f)\n",
    "# Loading the 100k training dataset\n",
    "with open(train_100k_path, 'rb') as f:\n",
    "    train_100k = plk.load(f)"
   ],
   "id": "e7b8e326e347162d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training the models\n",
    "\n",
    "In these cells, we will train the SVD and KNNBaseline models on both the 1M and 100k datasets.\n",
    "We will be using the best hyperparameters found online to avoid the need for hyperparameter tuning using grid search to avoid any possible OOM issues.\n",
    "\n",
    "We will also save the trained models to disk for later evaluation in the next notebook."
   ],
   "id": "a6281010cb9ec257"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Training SVD on 1M dataset\n",
    "\n",
    "In this cell, we will train the SVD model on the 1M dataset using the best hyperparameters found online and save the trained model to disk."
   ],
   "id": "20953a22a6c74b28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:22.923054929Z",
     "start_time": "2026-01-07T19:25:10.750208112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the best hyperparameters for SVD on 1M dataset\n",
    "svd_1m_params = {\n",
    "    'n_factors': 100,\n",
    "    'n_epochs': 20\n",
    "}\n",
    "\n",
    "# Initializing the SVD model on 1M dataset\n",
    "svd_1m = SVD(**svd_1m_params)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "svd_1m.fit(train_1m)\n",
    "\n",
    "# Saving the trained SVD model to disk\n",
    "svd_1m_path = pth.cwd().parent / 'models' / 'svd_1m.pkl'\n",
    "with open(svd_1m_path, 'wb') as f:\n",
    "    plk.dump(svd_1m, f)"
   ],
   "id": "87afc902a3506b9f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.1.1 Cleanup after training SVD on 1M dataset\n",
    "\n",
    "In this cell, we will delete the trained SVD model for the 1M dataset from memory and run garbage collection to free up memory."
   ],
   "id": "3594200efa0b06a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:23.019386835Z",
     "start_time": "2026-01-07T19:25:22.953025802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting the trained SVD model for 1M dataset to save memory\n",
    "del svd_1m\n",
    "\n",
    "#Deleting the training hyperparameters dictionary\n",
    "del svd_1m_params\n",
    "\n",
    "# Deleting the trained model path variable\n",
    "del svd_1m_path\n",
    "\n",
    "# calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "bc6b9b18ddf7e960",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 Training KNNBaseline on 1M dataset\n",
    "\n",
    "In this cell, we will train the KNNBaseline model on the 1M dataset using the best hyperparameters found online and save the trained model to disk."
   ],
   "id": "990cb2b6809f45f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:39.943888100Z",
     "start_time": "2026-01-07T19:25:23.021000624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the best hyperparameters for KNNBaseline on 1M dataset\n",
    "knn_1m_params = {\n",
    "    'k': 40,\n",
    "    'sim_options': {\n",
    "        'name': 'pearson_baseline',\n",
    "        'user_based': False # item-based collaborative filtering to save up memory space\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initializing the KNNBaseline model on 1M dataset\n",
    "knn_1m = KNNBaseline(**knn_1m_params)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "knn_1m.fit(train_1m)\n",
    "\n",
    "# Saving the trained KNNBaseline model to disk\n",
    "knn_1m_path = pth.cwd().parent / 'models' / 'knn_1m.pkl'\n",
    "with open(knn_1m_path, 'wb') as f:\n",
    "    plk.dump(knn_1m, f)"
   ],
   "id": "8d612491eb7e4f7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.2.1 Cleanup after training KNNBaseline on 1M dataset\n",
    "In this cell, we will delete the trained KNNBaseline model for the 1M dataset from memory and run garbage collection to free up memory."
   ],
   "id": "efb4a762dacac88d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:40.017642990Z",
     "start_time": "2026-01-07T19:25:39.949194665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting the trained KNNBaseline model for 1M dataset to save memory\n",
    "del knn_1m\n",
    "\n",
    "# Deleting the training hyperparameters dictionary\n",
    "del knn_1m_params\n",
    "\n",
    "# Deleting the trained model path variable\n",
    "del knn_1m_path\n",
    "\n",
    "# calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "4bfbb347f8550880",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Cleanup after training on 1M dataset\n",
    "\n",
    "In this cell, we will delete the 1M training dataset from memory and run garbage collection to free up memory."
   ],
   "id": "8d2c6f295e01706e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:40.096373150Z",
     "start_time": "2026-01-07T19:25:40.018360733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting the 1M training dataset to save memory\n",
    "del train_1m\n",
    "\n",
    "# calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "6df38603f3a77ff0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4 Training SVD on 100k dataset\n",
    "\n",
    "In this cell, we will train the SVD model on the 100k dataset using the best hyperparameters found online and save the trained model to disk."
   ],
   "id": "9ad046ba7fa12ff5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:40.646766282Z",
     "start_time": "2026-01-07T19:25:40.097908185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the best hyperparameters for SVD on 100k dataset\n",
    "svd_100k_params = {\n",
    "    'n_factors': 50,\n",
    "    'n_epochs': 15\n",
    "}\n",
    "\n",
    "# Initializing the SVD model on 100k dataset\n",
    "svd_100k = SVD(**svd_100k_params)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "svd_100k.fit(train_100k)\n",
    "\n",
    "# Saving the trained SVD model to disk\n",
    "svd_100k_path = pth.cwd().parent / 'models' / 'svd_100k.pkl'\n",
    "with open(svd_100k_path, 'wb') as f:\n",
    "    plk.dump(svd_100k, f)"
   ],
   "id": "2248c5cec7f1e14d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.4.1 Cleanup after training SVD on 100k dataset\n",
    "\n",
    "In this cell, we will delete the trained SVD model for the 100k dataset from memory and run garbage collection to free up memory."
   ],
   "id": "a95cdf02f885da7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:40.700684068Z",
     "start_time": "2026-01-07T19:25:40.652924701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting the trained SVD model for 100k dataset to save memory\n",
    "del svd_100k\n",
    "\n",
    "# Deleting the training hyperparameters dictionary\n",
    "del svd_100k_params\n",
    "\n",
    "# Deleting the trained model path variable\n",
    "del svd_100k_path\n",
    "\n",
    "# calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "5e168d06e904ef4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.5 Training KNNBaseline on 100k dataset\n",
    "\n",
    "In this cell, we will train the KNNBaseline model on the 100k dataset using the best hyperparameters found online and save the trained model to disk."
   ],
   "id": "a7224d33597da237"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:40.997447119Z",
     "start_time": "2026-01-07T19:25:40.702128927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining the best hyperparameters for KNNBaseline on 100k dataset\n",
    "knn_100k_params = {\n",
    "    'k': 30,\n",
    "    'sim_options': {\n",
    "        'name': 'pearson_baseline',\n",
    "        'user_based': True # We can use user-based collaborative filtering for smaller datasets without any risk of  OOM issues which results in better recommendations\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initializing the KNNBaseline model on 100k dataset\n",
    "knn_100k = KNNBaseline(**knn_100k_params)\n",
    "\n",
    "# Fitting the model to the training data\n",
    "knn_100k.fit(train_100k)\n",
    "\n",
    "# Saving the trained KNNBaseline model to disk\n",
    "knn_100k_path = pth.cwd().parent / 'models' / 'knn_100k.pkl'\n",
    "with open(knn_100k_path, 'wb') as f:\n",
    "    plk.dump(knn_100k, f)"
   ],
   "id": "2afc3879827c99e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3.5.1 Cleanup after training KNNBaseline on 100k dataset\n",
    "\n",
    "In this cell, we will delete the trained KNNBaseline model for the 100k dataset from memory and run garbage collection to free up memory."
   ],
   "id": "64545bce1b725079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:41.048064721Z",
     "start_time": "2026-01-07T19:25:40.998145269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting the trained KNNBaseline model for 100k dataset to save memory\n",
    "del knn_100k\n",
    "\n",
    "# Deleting the training hyperparameters dictionary\n",
    "del knn_100k_params\n",
    "\n",
    "# Deleting the trained model path variable\n",
    "del knn_100k_path\n",
    "\n",
    "# calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "8bd6a21e79516021",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.6 Cleanup after training on 100k dataset\n",
    "\n",
    "In this cell, we will delete the 100k training dataset from memory and run garbage collection to free up memory."
   ],
   "id": "350192ff99c7ceea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:41.094718611Z",
     "start_time": "2026-01-07T19:25:41.049572261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting the 100k training dataset to save memory\n",
    "del train_100k\n",
    "\n",
    "# calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "4d97922ac58873e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Jupiter notebook shutdown",
   "id": "449b39f1424feb43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:41.105949469Z",
     "start_time": "2026-01-07T19:25:41.096140526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Shutdown the Jupyter notebook kernel programmatically\n",
    "print(\"Shutting down the Jupyter notebook kernel for this notebook...\")\n",
    "IPython.get_ipython().kernel.do_shutdown(restart=False)"
   ],
   "id": "9cb86c7d52250ec1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down the Jupyter notebook kernel for this notebook...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we successfully built and trained recommendation models using the SVD and KNNBaseline algorithms from the Surprise library.\n",
    "We utilized the prepared datasets from the previous notebook and saved the trained models to disk for later evaluation.\n",
    "The next step will involve evaluating these models to assess their performance and effectiveness in making recommendations."
   ],
   "id": "c5ec418eeed9c792"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
