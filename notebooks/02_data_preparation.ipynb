{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this notebook, we will prepare the data for training and testing.\n",
    "Since we are using two datasets (1M and 32M), we will prepare both datasets separately.\n",
    "We will be using functions from the utils folder to help with data splitting and preparation and the DataFrames created in the previous notebook."
   ],
   "id": "ed277cf312c14a86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Utils folder path setup for imports\n",
    "\n",
    "Before we start we need to add the utils folder to the system path so that we can import the functions defined there."
   ],
   "id": "9569fecc0e7443d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:56.322889415Z",
     "start_time": "2026-01-07T19:24:56.316553030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path as pth\n",
    "import gc\n",
    "import IPython\n",
    "\n",
    "# Add the utils folder to the system path\n",
    "project_root = pth.cwd().parent # Should go from ML_Project/notebooks to ML_Project\n",
    "\n",
    "sys.path.insert(0,str(project_root))\n",
    "\n",
    "print(\"Utils folder added to system path for imports.\")"
   ],
   "id": "f2d9c2b8b88726d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils folder added to system path for imports.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Importing Necessary Libraries and Functions\n",
    "\n",
    "This cell imports all the necessary libraries and functions for data preparation."
   ],
   "id": "e28e8229b72d924a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:56.488495360Z",
     "start_time": "2026-01-07T19:24:56.326247358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from utils.preprocessing import format_rating_data, format_movies_data, load_surprise_dataset, split_surprise_dataset, load_movies_data, load_rating_data, merge_datasets\n",
    "from pathlib import Path as pth"
   ],
   "id": "4aac822b9a856411",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Load the Datasets\n",
    "\n",
    "This cell loads the movies and ratings datasets (32M and 1M) and prepares them for formatting to the correct format.\n",
    "This is done thanks to pre defined functions in the utils folder."
   ],
   "id": "6515c81279e803d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:58.647484002Z",
     "start_time": "2026-01-07T19:24:56.489430531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Declaring the 1M dataset file paths\n",
    "ratings_1m_path = pth.cwd().parent / 'data' / 'ml-1m' / 'ratings.dat'\n",
    "movies_1m_path = pth.cwd().parent / 'data' / 'ml-1m' / 'movies.dat'\n",
    "\n",
    "# Declaring the 100K dataset file paths\n",
    "ratings_100k_path = pth.cwd().parent / 'data' / 'ml-latest-small' / 'ratings.csv'\n",
    "movies_100k_path = pth.cwd().parent / 'data' / 'ml-latest-small' / 'movies.csv'\n",
    "\n",
    "# Loading the 1M dataset\n",
    "ratings_1m = load_rating_data(ratings_1m_path,'dat')\n",
    "movies_1m = load_movies_data(movies_1m_path,'dat')\n",
    "\n",
    "# Cleaning and formatting 1M dataset using predefined functions\n",
    "formatted_movies_1m = format_movies_data(movies_1m)\n",
    "formatted_ratings_1m = format_rating_data(ratings_1m)\n",
    "\n",
    "# Loading the 100K dataset\n",
    "ratings_100k = load_rating_data(ratings_100k_path, 'csv')\n",
    "movies_100k = load_movies_data(movies_100k_path, 'csv')\n",
    "\n",
    "# Cleaning and formatting 100K dataset using predefined functions\n",
    "formatted_movies_100k = format_movies_data(movies_100k)\n",
    "formatted_ratings_100k = format_rating_data(ratings_100k)\n",
    "\n",
    "# Merging datasets\n",
    "merged_1m = merge_datasets(formatted_movies_1m, formatted_ratings_1m)\n",
    "merged_100k = merge_datasets(formatted_movies_100k, formatted_ratings_100k)"
   ],
   "id": "3303db22f2a8018b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Memory Cleanup\n",
    "To optimize memory usage, we will delete the unformatted datasets and run garbage collection."
   ],
   "id": "2d25abf0400d03ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:58.725876043Z",
     "start_time": "2026-01-07T19:24:58.682180714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting unformatted datasets to save memory\n",
    "del ratings_1m\n",
    "del movies_1m\n",
    "del ratings_100k\n",
    "del movies_100k\n",
    "\n",
    "# Deleting formatted datasets to save memory\n",
    "del formatted_movies_1m\n",
    "del formatted_ratings_1m\n",
    "del formatted_movies_100k\n",
    "del formatted_ratings_100k\n",
    "\n",
    "# Deleting path variables\n",
    "del ratings_1m_path\n",
    "del movies_1m_path\n",
    "del ratings_100k_path\n",
    "del movies_100k_path\n",
    "\n",
    "# Running the garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "7865ba9f3713d5b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Verifying Loaded Datasets\n",
    "\n",
    "This cell will display the shape and the info of the merged dataframes of these 2 datasets to ensure they are loaded correctly."
   ],
   "id": "bf29bd2e0df20ca2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:58.789774730Z",
     "start_time": "2026-01-07T19:24:58.729298928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Displaying the shape of the merged 1M dataset\n",
    "print(\"1M Merged Dataset Shape:\", merged_1m.shape)\n",
    "\n",
    "# Displaying the shape of the merged 100K dataset\n",
    "print(\"100K Merged Dataset Shape:\", merged_100k.shape)\n",
    "\n",
    "# Displaying the info of the merged 1M dataset\n",
    "print(\"\\n1M Merged Dataset Info:\")\n",
    "merged_1m.info()\n",
    "\n",
    "# Displaying the info of the merged 100K dataset\n",
    "print(\"\\n100k Merged Dataset Info:\")\n",
    "merged_100k.info()"
   ],
   "id": "92d1bb7bf81505f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1M Merged Dataset Shape: (1000209, 5)\n",
      "100K Merged Dataset Shape: (100836, 5)\n",
      "\n",
      "1M Merged Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count    Dtype \n",
      "---  ------   --------------    ----- \n",
      " 0   movieId  1000209 non-null  int64 \n",
      " 1   title    1000209 non-null  object\n",
      " 2   genres   1000209 non-null  object\n",
      " 3   userId   1000209 non-null  int64 \n",
      " 4   rating   1000209 non-null  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 38.2+ MB\n",
      "\n",
      "100k Merged Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   movieId  100836 non-null  int64  \n",
      " 1   title    100836 non-null  object \n",
      " 2   genres   100836 non-null  object \n",
      " 3   userId   100836 non-null  int64  \n",
      " 4   rating   100836 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preparation for Training and Testing\n",
    "\n",
    "In this cell, we will prepare the datasets for training and testing using functions from the utils folder.\n",
    "We will start by formatting them to the surprise library format of Dataset instead of pandas DataFrame.\n",
    "Afterwards, we will split them into training and testing sets.\n",
    "The train test split will be 80% training and 20% testing."
   ],
   "id": "cb21ae7016f1ac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:59.796237993Z",
     "start_time": "2026-01-07T19:24:58.790760596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preparing the 1M dataset for the train test split\n",
    "surprise_1m = load_surprise_dataset(merged_1m)\n",
    "\n",
    "# Preparing the 100k dataset for the train test split\n",
    "surprise_100k = load_surprise_dataset(merged_100k)\n",
    "\n",
    "# Splitting the 1M dataset into training and testing sets\n",
    "train_1m, test_1m = split_surprise_dataset(surprise_1m, test_size=0.2)\n",
    "\n",
    "# Splitting the 100k dataset into training and testing sets\n",
    "train_100k, test_100k = split_surprise_dataset(surprise_100k, test_size=0.2)"
   ],
   "id": "3c627bedd8e005e1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Memory Cleanup After Preparation\n",
    "\n",
    "To optimize memory usage, we will delete the merged datasets and run garbage collection."
   ],
   "id": "196d1dc376100e56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:24:59.920625463Z",
     "start_time": "2026-01-07T19:24:59.803933676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting merged datasets to save memory\n",
    "del merged_1m\n",
    "del merged_100k\n",
    "# Deleting surprise datasets to save memory\n",
    "del surprise_1m\n",
    "del surprise_100k\n",
    "# Calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "7f04b07cf0df0e32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Saving prepared Data to Disk for access in the training notebook\n",
    "\n",
    "In this cell, we will save the prepared training and testing datasets to disk using the pickle format.\n",
    "This will allow us to easily load the datasets in the training notebook without having to repeat the preparation steps."
   ],
   "id": "311623a72ac2f14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:00.500238092Z",
     "start_time": "2026-01-07T19:24:59.924651365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle as plk\n",
    "\n",
    "# Defining the paths to save the prepared datasets\n",
    "train_1m_path = project_root / 'data' / 'prepared-1m' / 'train_1m.pkl'\n",
    "test_1m_path = project_root / 'data' / 'prepared-1m' / 'test_1m.pkl'\n",
    "train_100k_path = project_root / 'data' / 'prepared-100k' / 'train_100k.pkl'\n",
    "test_100k_path = project_root / 'data' / 'prepared-100k' / 'test_100k.pkl'\n",
    "\n",
    "# Saving the 1M training dataset\n",
    "with open(train_1m_path, 'wb') as f:\n",
    "    plk.dump(train_1m, f)\n",
    "# Saving the 1M testing dataset\n",
    "with open(test_1m_path, 'wb') as f:\n",
    "    plk.dump(test_1m, f)\n",
    "# Saving the 100k training dataset\n",
    "with open(train_100k_path, 'wb') as f:\n",
    "    plk.dump(train_100k, f)\n",
    "# Saving the 100k testing dataset\n",
    "with open(test_100k_path, 'wb') as f:\n",
    "    plk.dump(test_100k, f)"
   ],
   "id": "b3e132dbc644a886",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Final Cleanup\n",
    "After saving the prepared datasets, we will clean up the memory used for better resource management."
   ],
   "id": "35c579c99e16f6a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:00.693119608Z",
     "start_time": "2026-01-07T19:25:00.507519752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cleaning up the memory used by the training and testing datasets\n",
    "del train_1m\n",
    "del test_1m\n",
    "del train_100k\n",
    "del test_100k\n",
    "# Cleaning up any other variables that are no longer needed\n",
    "del project_root\n",
    "del train_1m_path\n",
    "del test_1m_path\n",
    "del train_100k_path\n",
    "del test_100k_path\n",
    "# Calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "b81ec9ff1706520d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Jupiter notebook shutdown",
   "id": "ab7da3649add12b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T19:25:00.722107138Z",
     "start_time": "2026-01-07T19:25:00.694858088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Shutdown the Jupyter notebook kernel programmatically\n",
    "print(\"Shutting down the Jupyter notebook kernel for this notebook...\")\n",
    "IPython.get_ipython().kernel.do_shutdown(restart=False)"
   ],
   "id": "e0e23920a01dce2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down the Jupyter notebook kernel for this notebook...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': False}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we successfully prepared the 1M and 100K datasets for training and testing.\n",
    "We formatted the datasets, split them into training and testing sets, and saved them to disk for easy access in the training notebook."
   ],
   "id": "188087eeabcb5a17"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
