{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "\n",
    "In this notebook, we will prepare the data for training and testing.\n",
    "Since we are using two datasets (1M and 32M), we will prepare both datasets separately.\n",
    "We will be using functions from the utils folder to help with data splitting and preparation and the DataFrames created in the previous notebook."
   ],
   "id": "ed277cf312c14a86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Utils folder path setup for imports\n",
    "\n",
    "Before we start we need to add the utils folder to the system path so that we can import the functions defined there."
   ],
   "id": "9569fecc0e7443d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:17:50.563858149Z",
     "start_time": "2026-01-07T17:17:50.537263849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path as pth\n",
    "import gc\n",
    "import IPython\n",
    "\n",
    "# Add the utils folder to the system path\n",
    "project_root = pth.cwd().parent # Should go from ML_Project/notebooks to ML_Project\n",
    "\n",
    "sys.path.insert(0,str(project_root))\n",
    "\n",
    "print(\"Utils folder added to system path for imports.\")"
   ],
   "id": "f2d9c2b8b88726d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils folder added to system path for imports.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Importing Necessary Libraries and Functions\n",
    "\n",
    "This cell imports all the necessary libraries and functions for data preparation."
   ],
   "id": "e28e8229b72d924a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:17:51.762085105Z",
     "start_time": "2026-01-07T17:17:51.724307573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from utils.preprocessing import format_rating_data, format_movies_data, load_surprise_dataset, split_surprise_dataset, load_movies_data, load_rating_data, merge_datasets\n",
    "from pathlib import Path as pth"
   ],
   "id": "4aac822b9a856411",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Load the Datasets\n",
    "\n",
    "This cell loads the movies and ratings datasets (32M and 1M) and prepares them for formatting to the correct format.\n",
    "This is done thanks to pre defined functions in the utils folder."
   ],
   "id": "6515c81279e803d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:18:11.487927511Z",
     "start_time": "2026-01-07T17:17:53.658791842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Declaring the 1M dataset file paths\n",
    "ratings_1m_path = pth.cwd().parent / 'data' / 'ml-1m' / 'ratings.dat'\n",
    "movies_1m_path = pth.cwd().parent / 'data' / 'ml-1m' / 'movies.dat'\n",
    "\n",
    "# Declaring the 100K dataset file paths\n",
    "ratings_100k_path = pth.cwd().parent / 'data' / 'ml-32m' / 'ratings.csv'\n",
    "movies_100k_path = pth.cwd().parent / 'data' / 'ml-32m' / 'movies.csv'\n",
    "\n",
    "# Loading the 1M dataset\n",
    "ratings_1m = load_rating_data(ratings_1m_path,'dat')\n",
    "movies_1m = load_movies_data(movies_1m_path,'dat')\n",
    "\n",
    "# Cleaning and formatting 1M dataset using predefined functions\n",
    "formatted_movies_1m = format_movies_data(movies_1m)\n",
    "formatted_ratings_1m = format_rating_data(ratings_1m)\n",
    "\n",
    "# Loading the 100K dataset\n",
    "ratings_100k = load_rating_data(ratings_100k_path, 'csv')\n",
    "movies_100k = load_movies_data(movies_100k_path, 'csv')\n",
    "\n",
    "# Cleaning and formatting 100K dataset using predefined functions\n",
    "formatted_movies_100k = format_movies_data(movies_100k)\n",
    "formatted_ratings_100k = format_rating_data(ratings_100k)\n",
    "\n",
    "# Merging datasets\n",
    "merged_1m = merge_datasets(formatted_movies_1m, formatted_ratings_1m)\n",
    "merged_100k = merge_datasets(formatted_movies_100k, formatted_ratings_100k)"
   ],
   "id": "3303db22f2a8018b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Memory Cleanup\n",
    "To optimize memory usage, we will delete the unformatted datasets and run garbage collection."
   ],
   "id": "2d25abf0400d03ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:18:16.345021708Z",
     "start_time": "2026-01-07T17:18:16.274839957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Deleting unformatted datasets to save memory\n",
    "del ratings_1m\n",
    "del movies_1m\n",
    "del ratings_100k\n",
    "del movies_100k\n",
    "\n",
    "# Deleting formatted datasets to save memory\n",
    "del formatted_movies_1m\n",
    "del formatted_ratings_1m\n",
    "del formatted_movies_100k\n",
    "del formatted_ratings_100k\n",
    "\n",
    "# Deleting path variables\n",
    "del ratings_1m_path\n",
    "del movies_1m_path\n",
    "del ratings_100k_path\n",
    "del movies_100k_path\n",
    "\n",
    "# Running the garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "7865ba9f3713d5b2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Verifying Loaded Datasets\n",
    "\n",
    "This cell will display the shape and the info of the merged dataframes of these 2 datasets to ensure they are loaded correctly."
   ],
   "id": "bf29bd2e0df20ca2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-07T17:18:55.261990119Z",
     "start_time": "2026-01-07T17:18:55.195376647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Displaying the shape of the merged 1M dataset\n",
    "print(\"1M Merged Dataset Shape:\", merged_1m.shape)\n",
    "\n",
    "# Displaying the shape of the merged 100K dataset\n",
    "print(\"100K Merged Dataset Shape:\", merged_100k.shape)\n",
    "\n",
    "# Displaying the info of the merged 1M dataset\n",
    "print(\"\\n1M Merged Dataset Info:\")\n",
    "merged_1m.info()\n",
    "\n",
    "# Displaying the info of the merged 100K dataset\n",
    "print(\"\\n100k Merged Dataset Info:\")\n",
    "merged_100k.info()"
   ],
   "id": "92d1bb7bf81505f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1M Merged Dataset Shape: (1000209, 5)\n",
      "32M Merged Dataset Shape: (32000204, 5)\n",
      "\n",
      "1M Merged Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count    Dtype \n",
      "---  ------   --------------    ----- \n",
      " 0   movieId  1000209 non-null  int64 \n",
      " 1   title    1000209 non-null  object\n",
      " 2   genres   1000209 non-null  object\n",
      " 3   userId   1000209 non-null  int64 \n",
      " 4   rating   1000209 non-null  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 38.2+ MB\n",
      "\n",
      "32M Merged Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32000204 entries, 0 to 32000203\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Dtype  \n",
      "---  ------   -----  \n",
      " 0   movieId  int64  \n",
      " 1   title    object \n",
      " 2   genres   object \n",
      " 3   userId   int64  \n",
      " 4   rating   float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preparation for Training and Testing\n",
    "\n",
    "In this cell, we will prepare the datasets for training and testing using functions from the utils folder.\n",
    "We will start by formatting them to the surprise library format of Dataset instead of pandas DataFrame.\n",
    "Afterwards, we will split them into training and testing sets.\n",
    "The train test split will be 80% training and 20% testing."
   ],
   "id": "cb21ae7016f1ac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-07T17:25:10.845747696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preparing the 1M dataset for the train test split\n",
    "surprise_1m = load_surprise_dataset(merged_1m)\n",
    "\n",
    "# Preparing the 100k dataset for the train test split\n",
    "surprise_100k = load_surprise_dataset(merged_100k)\n",
    "\n",
    "# Splitting the 1M dataset into training and testing sets\n",
    "train_1m, test_1m = split_surprise_dataset(surprise_1m, test_size=0.2)\n",
    "\n",
    "# Splitting the 100k dataset into training and testing sets\n",
    "train_32m, test_32m = split_surprise_dataset(surprise_100k, test_size=0.2)"
   ],
   "id": "3c627bedd8e005e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1 Memory Cleanup After Preparation\n",
    "\n",
    "To optimize memory usage, we will delete the merged datasets and run garbage collection."
   ],
   "id": "196d1dc376100e56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Deleting merged datasets to save memory\n",
    "del merged_1m\n",
    "del merged_100k\n",
    "# Calling garbage collector to free up memory\n",
    "gc.collect()"
   ],
   "id": "7f04b07cf0df0e32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
